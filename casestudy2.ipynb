{"cells": [{"cell_type": "code", "execution_count": null, "id": "26ef7e10-f41d-4c1d-bb9b-40832437228e", "metadata": {}, "outputs": [], "source": "# import modules\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, when, avg, from_json, lag, round, stddev, unix_timestamp, lit\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nimport google.cloud.logging\nimport logging\n\n# variables setup\nGCS_BUCKET_SOURCE = \"gs://sampple-bkt-13022025/patients_data/*.json\"\nBQ_TABLE = \"avd-group-gcp-1111.gold_dataset.patients_insights\"\nGCS_BUCKET_INVALID = \"gs://sampple-bkt-13022025/invalid/\"\nGCS_TEMP_BUCKET = \"gs://sampple-bkt-13022025/temp/\"\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"HealthcareDataProcessing\") \\\n    .getOrCreate()\n\n# Initialize Google Cloud Logging\nlogging_client = google.cloud.logging.Client()\nlogging_client.setup_logging()\nlogger = logging.getLogger('healthcare-pipeline')\n\n# Logging helper function\ndef log_pipeline_step(step, message, level='INFO'):\n    if level == 'INFO':\n        logger.info(f\"Step: {step}, Message: {message}\")\n    elif level == 'ERROR':\n        logger.error(f\"Step: {step}, Error: {message}\")\n    elif level == 'WARNING':\n        logger.warning(f\"Step: {step}, Warning: {message}\")\n        \n# Define schema for JSON data\nschema = StructType([\n    StructField(\"patient_id\", StringType(), True),\n    StructField(\"heart_rate\", IntegerType(), True),\n    StructField(\"blood_pressure\", IntegerType(), True),\n    StructField(\"temperature\", DoubleType(), True),\n    StructField(\"timestamp\", StringType(), True)\n])\n\n# Function to validate incoming data\ndef validate_data(df):\n    log_pipeline_step(\"Data Validation\", \"Starting data validation.\")\n    \n    validated_df = df.withColumn(\"is_valid\", when((col(\"heart_rate\") > 40) & (col(\"heart_rate\") < 200) & \n                                                  (col(\"blood_pressure\") > 50) & (col(\"blood_pressure\") < 200) & \n                                                  (col(\"temperature\") > 35.0) & (col(\"temperature\") < 42.0), True).otherwise(False))\n    \n    valid_records = validated_df.filter(col(\"is_valid\") == True)\n    invalid_records = validated_df.filter(col(\"is_valid\") == False)\n    \n    log_pipeline_step(\"Data Validation\", f\"Valid records: {valid_records.count()}, Invalid records: {invalid_records.count()}\")\n    return valid_records, invalid_records\n\n\n# Main processing function\ndef process_data():\n    try:\n        # Step 1: Read raw data from GCS\n        log_pipeline_step(\"Data Ingestion\", \"Reading raw data from GCS.\")\n        df = spark.read.schema(schema).json(GCS_BUCKET_SOURCE)\n        \n        # Step 2: Validate data\n        valid_df, invalid_df = validate_data(df)\n        \n        # Step 3: Log invalid records (for auditing)\n        if invalid_df.count() > 0:\n            log_pipeline_step(\"Invalid Data\", \"Found invalid records.\", level='WARNING')\n            invalid_df.write.mode(\"append\").json(GCS_BUCKET_INVALID)\n            \n        # Step 4: Data Transformation - Aggregate by patient_id\n        log_pipeline_step(\"Data Transformation\", \"Aggregating data by patient_id.\")\n        df_agg = valid_df.groupBy(\"patient_id\").agg(\n                                                    round(avg(\"heart_rate\"), 2).alias(\"avg_heart_rate\"),\n                                                    round(avg(\"blood_pressure\"), 2).alias(\"avg_blood_pressure\"),\n                                                    round(avg(\"temperature\"), 2).alias(\"avg_temperature\")\n                                                )\n        \n        # Step 5: Calculate standard deviation for heart rate and blood pressure for each patient\n        log_pipeline_step(\"Data Transformation\", \"Calculating standard deviation for heart rate and blood pressure.\")\n        df_stddev = valid_df.groupBy(\"patient_id\").agg(\n                                                        stddev(\"heart_rate\").alias(\"stddev_heart_rate\"),\n                                                        stddev(\"blood_pressure\").alias(\"stddev_blood_pressure\")\n                                                    )\n        \n        # Step 6: Join the aggregated data with standard deviation metrics\n        log_pipeline_step(\"Data Transformation\", \"Joining aggregated data with standard deviation metrics.\")\n        df_joined = df_agg.join(df_stddev, on=\"patient_id\")\n        \n        # Step 8: Flag patients with high average heart rate or high heart rate variation\n        log_pipeline_step(\"Data Transformation\", \"Flagging high-risk patients.\")\n        df_joined = df_joined.withColumn(\"risk_category\", \n                                                        when(col(\"avg_heart_rate\") > 100, \"High Risk\")\n                                                        .when(col(\"stddev_heart_rate\") > 15, \"Moderate Risk\")\n                                                        .otherwise(\"Low Risk\"))\n        \n        # Step 9: Write the aggregated data with risk categorization to BigQuery\n        log_pipeline_step(\"Data Write\", \"Writing aggregated and transformed data to BigQuery.\")\n        df_joined.write \\\n                    .format(\"bigquery\") \\\n                    .option(\"table\", BQ_TABLE) \\\n                    .option(\"temporaryGcsBucket\", GCS_TEMP_BUCKET) \\\n                    .mode(\"append\") \\\n                    .save()\n\n    \n    except Exception as e:\n        log_pipeline_step(\"Processing Error\", str(e), level='ERROR')\n        raise e\n\n\n# Execute the processing function\nif __name__ == \"__main__\":\n    log_pipeline_step(\"Pipeline Start\", \"Healthcare data processing pipeline initiated.\")\n    process_data()\n    log_pipeline_step(\"Pipeline End\", \"Healthcare data processing pipeline completed.\")"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}